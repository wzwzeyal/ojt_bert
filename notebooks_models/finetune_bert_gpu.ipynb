{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BotaOAs1k3up",
        "outputId": "7ee2e50e-140e-4c3b-c62e-bbbe4e527776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ojt_bert' already exists and is not an empty directory.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  ! git clone https://github.com/wzwzeyal/ojt_bert.git\n",
        "  ! pip install transformers\n",
        "  SENTIMENT_DATA_PATH = './ojt_bert/data/for_sentiment'\n",
        "else:\n",
        "  SENTIMENT_DATA_PATH = '../data/for_sentiment'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "iQiT6Euak3uu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xEpSc4lXk3uv",
        "outputId": "9e8ab8ee-2ddc-4ce4-f478-07d2c6b1b7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "y4MbE9Jjk3uw"
      },
      "outputs": [],
      "source": [
        "nof_classes = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ShkHPisYk3uw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifierModule(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, bert_model_name, freeze_bert=False, d_in=768, h=50, d_out=nof_classes):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifierModule, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = d_in, h, nof_classes\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "d0rpZ3Enk3uy"
      },
      "outputs": [],
      "source": [
        "class BertClassifer():\n",
        "    def __init__(self, max_len, batch_size, bert_model_name, epochs=4):\n",
        "        self.max_len  = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.bert_model_name = bert_model_name#initialize_model(bert_model_name, epochs)\n",
        "        #self.train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        \n",
        "    # Create a function to tokenize a set of texts\n",
        "    def preprocessing_for_bert(data):\n",
        "        \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "        @param    data (np.array): Array of texts to be processed.\n",
        "        @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "        @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                    tokens should be attended to by the model.\n",
        "        \"\"\"\n",
        "        # Create empty lists to store outputs\n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "\n",
        "        # For every sentence...\n",
        "        for sent in data:\n",
        "            # `encode_plus` will:\n",
        "            #    (1) Tokenize the sentence\n",
        "            #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "            #    (3) Truncate/Pad sentence to max length\n",
        "            #    (4) Map tokens to their IDs\n",
        "            #    (5) Create attention mask\n",
        "            #    (6) Return a dictionary of outputs\n",
        "            encoded_sent = tokenizer.encode_plus(\n",
        "                text=sent,#text_preprocessing(sent),  # Preprocess sentence\n",
        "                add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "                max_length= self.max_len,                  # Max length to truncate/pad\n",
        "                #  The `pad_to_max_length` argument is deprecated and will be removed in a future version, \n",
        "                # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \n",
        "                # or use `padding='max_length'` to pad to a max length. \n",
        "                # In this case, you can give a specific length with `max_length` \n",
        "                # (e.g. `max_length=45`) or leave max_length to None to \n",
        "                # pad to the maximal input size of the model (e.g. 512 for Bert).\n",
        "\n",
        "                # pad_to_max_length=True,         # Pad sentence to max length\n",
        "                padding='max_length',\n",
        "                #return_tensors='pt',           # Return PyTorch tensor\n",
        "                return_attention_mask=True      # Return attention mask\n",
        "                )\n",
        "            \n",
        "            # Add the outputs to the lists\n",
        "            input_ids.append(encoded_sent.get('input_ids'))\n",
        "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "        attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "        return input_ids, attention_masks \n",
        "        \n",
        "    def initialize_model(self, train_data, train_sampler, epochs=4):\n",
        "        \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "        \"\"\"\n",
        "        # Instantiate Bert Classifier\n",
        "        self.bert_classifier = BertClassifierModule(bert_model_name, freeze_bert=False)\n",
        "\n",
        "        # Tell PyTorch to run the model on GPU\n",
        "        self.bert_classifier.to(device)\n",
        "\n",
        "        # Create the optimizer\n",
        "        self.optimizer = AdamW(self.bert_classifier.parameters(),\n",
        "                        lr=5e-5,    # Default learning rate\n",
        "                        eps=1e-8    # Default epsilon value\n",
        "                        )\n",
        "\n",
        "        self.train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "        # Total number of training steps\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # Set up the learning rate scheduler\n",
        "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer,\n",
        "                                                    num_warmup_steps=0, # Default value\n",
        "                                                    num_training_steps=total_steps)\n",
        "\n",
        "\n",
        "    def prepare_dataset():\n",
        "        pass\n",
        "    \n",
        "    def evaluate(model, val_dataloader):\n",
        "        \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "        on our validation set.\n",
        "        \"\"\"\n",
        "        # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "        # the test time.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables\n",
        "        val_accuracy = []\n",
        "        val_loss = []\n",
        "\n",
        "        # For each batch in our validation set...\n",
        "        for batch in val_dataloader:\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Compute logits\n",
        "            with torch.no_grad():\n",
        "                logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.loss_fn(logits, b_labels)\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            # Get the predictions\n",
        "            preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "            # Calculate the accuracy rate\n",
        "            accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "            val_accuracy.append(accuracy)\n",
        "\n",
        "        # Compute the average accuracy and loss over the validation set.\n",
        "        val_loss = np.mean(val_loss)\n",
        "        val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    #def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    def train(self, val_dataloader=None, epochs=4, evaluation=False):\n",
        "        \"\"\"Train the BertClassifier model.\n",
        "        \"\"\"\n",
        "        # Start training loop\n",
        "        print(\"Start training...\\n\")\n",
        "\n",
        "        best_accuracy = 0\n",
        "\n",
        "        for epoch_i in range(epochs):\n",
        "            # =======================================\n",
        "            #               Training\n",
        "            # =======================================\n",
        "            # Print the header of the result table\n",
        "            print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "            print(\"-\"*70)\n",
        "\n",
        "            # Measure the elapsed time of each epoch\n",
        "            t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "            # Reset tracking variables at the beginning of each epoch\n",
        "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "            # Put the model into the training mode\n",
        "            self.bert_classifier.train()\n",
        "\n",
        "            # For each batch of training data...\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                batch_counts +=1\n",
        "                # Load batch to GPU\n",
        "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "                # Zero out any previously calculated gradients\n",
        "                self.bert_classifier.zero_grad()\n",
        "\n",
        "                # Perform a forward pass. This will return logits.\n",
        "                logits = self.bert_classifier(b_input_ids, b_attn_mask)\n",
        "\n",
        "                # Compute loss and accumulate the loss values\n",
        "                loss = self.loss_fn(logits, b_labels)\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Perform a backward pass to calculate gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "                torch.nn.utils.clip_grad_norm_(self.bert_classifier.parameters(), 1.0)\n",
        "\n",
        "                # Update parameters and the learning rate\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "\n",
        "                # Print the loss values and time elapsed for every 20 batches\n",
        "                if (step % 20 == 0 and step != 0) or (step == len(self.train_dataloader) - 1):\n",
        "                    # Calculate time elapsed for 20 batches\n",
        "                    time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                    # Print training results\n",
        "                    print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                    # Reset batch tracking variables\n",
        "                    batch_loss, batch_counts = 0, 0\n",
        "                    t0_batch = time.time()\n",
        "\n",
        "                \n",
        "            \n",
        "            # Calculate the average loss over the entire training data\n",
        "            avg_train_loss = total_loss / len(self.train_dataloader)\n",
        "\n",
        "            print(\"-\"*70)\n",
        "            # =======================================\n",
        "            #               Evaluation\n",
        "            # =======================================\n",
        "            if evaluation == True:\n",
        "                # After the completion of each training epoch, measure the model's performance\n",
        "                # on our validation set.\n",
        "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "                # Print performance over the entire training data\n",
        "                time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "                print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\", end='')\n",
        "\n",
        "                #-- Save best model (early stopping):\n",
        "                if val_accuracy > best_accuracy:\n",
        "                    best_accuracy = val_accuracy\n",
        "                    try   : torch.save(model.state_dict(), best_model_name)\n",
        "                    except: pass\n",
        "                    print(' <-- Checkpoint!')\n",
        "                else:\n",
        "                    print('')\n",
        "\n",
        "                print(\"-\"*70)\n",
        "            print(\"\\n\")\n",
        "        \n",
        "        print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "U9z591NWk3u1"
      },
      "outputs": [],
      "source": [
        "train_morph_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/train_morph_df.gz')\n",
        "train_token_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/train_token_df.gz')\n",
        "val_morph_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/val_morph_df.gz')\n",
        "val_token_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/val_token_df.gz')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,#text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            #  The `pad_to_max_length` argument is deprecated and will be removed in a future version, \n",
        "            # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \n",
        "            # or use `padding='max_length'` to pad to a max length. \n",
        "            # In this case, you can give a specific length with `max_length` \n",
        "            # (e.g. `max_length=45`) or leave max_length to None to \n",
        "            # pad to the maximal input size of the model (e.g. 512 for Bert).\n",
        "\n",
        "            # pad_to_max_length=True,         # Pad sentence to max length\n",
        "            padding='max_length',\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "hk3HKopOlXUJ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN=130\n",
        "batch_size=32\n",
        "bert_model_name = 'avichr/heBERT_sentiment_analysis'"
      ],
      "metadata": {
        "id": "KovFj_BKlNDn"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "y00N0-g2k3u2"
      },
      "outputs": [],
      "source": [
        "X_train = train_token_df.comment_clean.values\n",
        "y_train = train_token_df.label\n",
        "\n",
        "X_val = val_token_df.comment_clean.values\n",
        "y_val = val_token_df.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "pdm9f1pCk3u3"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "m9zyNP50k3u3"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "YLf41dBIk3u4"
      },
      "outputs": [],
      "source": [
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "val_labels = torch.tensor(y_val.values)\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBqltY3Zk3u4",
        "outputId": "7ceefb8e-38df-4afa-aa7b-0b8a1fd968ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at avichr/heBERT_sentiment_analysis were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.555687   |     -      |     -     |   28.99  \n"
          ]
        }
      ],
      "source": [
        "bert_classifier = BertClassifer(MAX_LEN, batch_size, bert_model_name)\n",
        "bert_classifier.initialize_model(train_data, train_sampler);\n",
        "bert_classifier.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRxjWIqGk3u5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-peQAyXzk3u5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYtnFIpmk3u6"
      },
      "outputs": [],
      "source": [
        "MAX_LEN=130\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH2KNJePk3u6"
      },
      "outputs": [],
      "source": [
        "train_morph_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/train_morph_df.gz')\n",
        "train_token_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/train_token_df.gz')\n",
        "val_morph_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/val_morph_df.gz')\n",
        "val_token_df = pd.read_csv(f'{SENTIMENT_DATA_PATH}/val_token_df.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM0R69Mqk3u6"
      },
      "outputs": [],
      "source": [
        "train_token_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn7yf5S1k3u7"
      },
      "outputs": [],
      "source": [
        "bert_model_name = 'avichr/heBERT_sentiment_analysis'\n",
        "#bert_model_name = 'Norod78/distilgpt2-base-pretrained-he'\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9fzuvyTk3u7"
      },
      "outputs": [],
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,#text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            #  The `pad_to_max_length` argument is deprecated and will be removed in a future version, \n",
        "            # use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \n",
        "            # or use `padding='max_length'` to pad to a max length. \n",
        "            # In this case, you can give a specific length with `max_length` \n",
        "            # (e.g. `max_length=45`) or leave max_length to None to \n",
        "            # pad to the maximal input size of the model (e.g. 512 for Bert).\n",
        "\n",
        "            # pad_to_max_length=True,         # Pad sentence to max length\n",
        "            padding='max_length',\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VktKj9PYk3u7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hC4uBu3rk3u7"
      },
      "outputs": [],
      "source": [
        "# perform for token\n",
        "\n",
        "X_train = train_token_df.comment_clean.values\n",
        "y_train = train_token_df.label\n",
        "\n",
        "X_val = val_token_df.comment_clean.values\n",
        "y_val = val_token_df.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5NWi01Fk3u8"
      },
      "outputs": [],
      "source": [
        "#train_morph_df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owbI0zSGk3u8"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WFcd6O4k3u8"
      },
      "outputs": [],
      "source": [
        "X_train[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MerPg-PFk3u8"
      },
      "outputs": [],
      "source": [
        "test = [X_train[0]]\n",
        "vall = [X_val[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-wCCzRk3u9"
      },
      "outputs": [],
      "source": [
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-MblYfWk3u9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train.values)\n",
        "val_labels = torch.tensor(y_val.values)\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kj1Fh0Mk3u9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "nof_classes = 3\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifierModule(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifierModule, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, nof_classes\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxZnx34Sk3u9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def initialize_model(self, epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifierModule(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Wq2H1Gk3u-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "best_model_name = f'./best_model_state.pt'\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "             \n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\", end='')\n",
        "\n",
        "            #-- Save best model (early stopping):\n",
        "            if val_accuracy > best_accuracy:\n",
        "              best_accuracy = val_accuracy\n",
        "              try   : torch.save(model.state_dict(), best_model_name)\n",
        "              except: pass\n",
        "              print(' <-- Checkpoint!')\n",
        "            else:\n",
        "              print('')\n",
        "\n",
        "\n",
        "            \n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coE-kYJkk3u-"
      },
      "outputs": [],
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=5)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87S9PDlBk3u-"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import files\n",
        "\n",
        "    files.download(best_model_name)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d4a6d3944a949846f4664787b256241dcb769797b6827641bf384da575c62243"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "finetune_bert_gpu.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}